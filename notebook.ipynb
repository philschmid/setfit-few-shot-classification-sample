{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outperform OpenAI GPT-3 for classifiation with SetFit\n",
    "\n",
    "In many Machine Learning applications, the amount of available labeled data is a barrier to producing a high-performing model. \n",
    "In the last 2 years developments have shown that you can overcome this data limitation by using Large Language Models, like [OpenAI GPT-3](https://openai.com/blog/gpt-3-apps/) together wit a *few* examples as prompts at inference time to achieve good results. These developments are improving the missing labeled data situation but are introducing a new problem, which is the access and cost of Large Language Models. \n",
    "\n",
    "But a group of research led by [Intel Labs](https://www.intel.com/content/www/us/en/research/overview.html) and the [UKP Lab](https://www.informatik.tu-darmstadt.de/ukp/ukp_home/index.en.jsp), [Hugging Face](https://huggingface.co/) released an new approach, called \"SetFit\" (https://arxiv.org/abs/2209.11055), that can be used to create high accuracte text-classification models with limited labeled data. \n",
    "SetFit is outperforming GPT-3 in 7 out of 11 tasks, while being 1600x smaller. \n",
    "\n",
    "![setfit-vs-gpt3](setfit-vs-gpt3.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this blog, you will learn how to use [SetFit](https://github.com/huggingface/setfit) to create a text-classification model with only a `8` labeled samples per class, or `32` samples in total. You will also learn how to improve your model by using hyperparamter tuning. \n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Setup Development Environment](#1-setup-development-environment)\n",
    "2. [Create Dataset](#2-create-dataset)\n",
    "3. [Fine-Tune Classifier with SetFit](#3-fine-tune-classifier-with-setfit)\n",
    "4. [Use Hyperparameter search to optimize results](#4-use-hyperparameter-search-to-optimize-results)\n",
    "\n",
    "## Why SetFit is better\n",
    "\n",
    "Compared to other few-shot learning methods, SetFit has several unique features:\n",
    "\n",
    "üó£ No prompts or verbalisers: Current techniques for few-shot fine-tuning require handcrafted prompts. SetFit dispenses with prompts altogether by generating rich embeddings directly from text examples.\n",
    "üèé Fast to train: SetFit doesn't require large-scale models like T0 or GPT-3 to achieve high accuracy.\n",
    "üåé Multilingual support: SetFit can be used with any Sentence Transformer on the Hub.\n",
    "\n",
    "---\n",
    "\n",
    "Now we know why SetFit is amazing, let's get started. üöÄ\n",
    "\n",
    "_Note: This tutorial was created and run on a g4dn.xlarge AWS EC2 Instance including a NVIDIA T4._\n",
    "\n",
    "\n",
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install the Hugging Face Libraries, including SetFit. Running the following cell will install all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install setfit[optuna]==0.3.0 datasets -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Dataset\n",
    "\n",
    "We are going to use the [ag_news](https://huggingface.co/datasets/ag_news) dataset, which a news article classification dataset with `4` classes: World (0), Sports (1), Business (2), Sci/Tech (3).\n",
    "\n",
    "The test split of the dataset contains `7600` examples, which is will be used to evaluate our model. The train split contains `120000` examples, which is a nice amount of data for fine-tuning a regular model. \n",
    "\n",
    "But to shwocase SetFit, we wanto to create a dataset with only a `8` labeled samples per class, or `32` data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/ubuntu/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca8eb32a7ec4823bfca09ca2c265871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7fcfce3a8ca0> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc9a5b0857d42cbadec9c27b2756362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1987cdf0d5b3431e813d2f6ea801fff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a00c2eddff146bab6c5c141e9487f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69aae0f171b245429a49e73bab0aa408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset,concatenate_datasets\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# create train dataset\n",
    "seed=20\n",
    "labels = 4\n",
    "samples_per_label = 8\n",
    "sampled_datasets = []\n",
    "# find the number of samples per label\n",
    "for i in range(labels):\n",
    "    sampled_datasets.append(dataset[\"train\"].filter(lambda x: x[\"label\"] == i).shuffle(seed=seed).select(range(samples_per_label)))\n",
    "\n",
    "# concatenate the sampled datasets\n",
    "train_dataset = concatenate_datasets(sampled_datasets)\n",
    "\n",
    "# create test dataset\n",
    "test_dataset = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune Classifier with SetFit\n",
    "\n",
    "When using SetFit we first fine-tune a Sentence Transformer model using our labeled data and contrastive training, where positive and negative pairs are created by in-class and out-class selection. \n",
    "The second step a classification head is trained on the encoded embeddings with their respective class labels. \n",
    "\n",
    "\n",
    "![train-diagram](setfit_diagram_process.png)\n",
    "\n",
    "\n",
    "As Sentence Transformers we are going to use [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2). (you could replace the model with any available sentence transformer on hf.co).\n",
    "\n",
    "The Python [SetFit](https://github.com/huggingface/setfit) package is implementing useful classes and functions to make the fine-tuning process straightforward and easy. Similar to the Hugging Face [Trainer](https://huggingface.co/transformers/main_classes/trainer.html) class, SetFits implmenets the `SetFitTrainer` class is responsible for the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 1280\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 20\n",
      "  Total train batch size = 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f370bc10f942de8e6daae64639ca74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5c1c6d77f1429b91dc5af8363604e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model used: sentence-transformers/all-mpnet-base-v2\n",
      "train dataset: 32 samples\n",
      "accuracy: 0.8647368421052631\n"
     ]
    }
   ],
   "source": [
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "# Load a SetFit model from Hub\n",
    "model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = SetFitModel.from_pretrained(model_id)\n",
    "\n",
    "# Create trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    metric=\"accuracy\",\n",
    "    batch_size=64,\n",
    "    num_iterations=20, # The number of text pairs to generate for contrastive learning\n",
    "    num_epochs=1, # The number of epochs to use for constrastive learning\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "print(f\"model used: {model_id}\")\n",
    "print(f\"train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"accuracy: {metrics['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Hyperparameter search to optimize result\n",
    "\n",
    "The `SetFitTrainer` provides a `hyperparameter_search()` method that you can use to find the perefect hyperparameters for the data. `SetFit` is leveraging `optuna` under the hood to perform the hyperparameter search. To use the hyperparameter search, we need to define a `model_init` method, which creates our model for every \"run\" and a `hp_space` method that defines the hyperparameter search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "# model specfic hyperparameters\n",
    "def model_init(params):\n",
    "    params = params or {}\n",
    "    max_iter = params.get(\"max_iter\", 100)\n",
    "    solver = params.get(\"solver\", \"liblinear\")\n",
    "    model_id = params.get(\"model_id\", \"sentence-transformers/all-mpnet-base-v2\")\n",
    "    model_params = {\n",
    "        \"head_params\": {\n",
    "            \"max_iter\": max_iter,\n",
    "            \"solver\": solver,\n",
    "        }\n",
    "    }\n",
    "    return SetFitModel.from_pretrained(model_id, **model_params)\n",
    "\n",
    "# training hyperparameters\n",
    "def hp_space(trial): \n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"num_epochs\": trial.suggest_int(\"num_epochs\", 1, 5),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32]),\n",
    "        \"num_iterations\": trial.suggest_categorical(\"num_iterations\", [5, 10, 20, 40, 80]),\n",
    "        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
    "        \"max_iter\": trial.suggest_int(\"max_iter\", 50, 300),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"newton-cg\", \"lbfgs\", \"liblinear\"]),\n",
    "        \"model_id\": trial.suggest_categorical(\n",
    "            \"model_id\",\n",
    "            [\n",
    "                \"sentence-transformers/all-mpnet-base-v2\",\n",
    "                \"sentence-transformers/all-MiniLM-L12-v1\",\n",
    "            ],\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = SetFitTrainer(\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    model_init=model_init,\n",
    ")\n",
    "\n",
    "best_run = trainer.hyperparameter_search(direction=\"maximize\", hp_space=hp_space, n_trials=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `100` trials (runs) the bes model was found with the following hyperparameters:\n",
    "\n",
    "```\n",
    "{'learning_rate': 2.2041595048800003e-05, 'num_epochs': 2, 'batch_size': 64, 'num_iterations': 20, 'seed': 34, 'max_iter': 182, 'solver': 'lbfgs', 'model_id': 'sentence-transformers/all-mpnet-base-v2'}\n",
    "```\n",
    "\n",
    "Achieving an accuracy of `0.873421052631579`, which is 1.1% better than the model we trained without hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 2.2041595048800003e-05,\n",
       " 'num_epochs': 2,\n",
       " 'batch_size': 64,\n",
       " 'num_iterations': 20,\n",
       " 'seed': 34,\n",
       " 'max_iter': 182,\n",
       " 'solver': 'lbfgs',\n",
       " 'model_id': 'sentence-transformers/all-mpnet-base-v2'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run.hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, we have found the perfect hyperparameters we need to run a last training using those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 1280\n",
      "  Num epochs = 2\n",
      "  Total optimization steps = 20\n",
      "  Total train batch size = 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da3bc1e620a4b22bbdf64621616f992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359425287f25474ca41810d6b5f93074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d780572471e14c109b0b00f4c9fa2c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model used: sentence-transformers/all-mpnet-base-v2\n",
      "train dataset: 32 samples\n",
      "accuracy: 0.873421052631579\n"
     ]
    }
   ],
   "source": [
    "trainer.apply_hyperparameters(best_run.hyperparameters, final_model=True)\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "print(f\"model used: {best_run.hyperparameters['model_id']}\")\n",
    "print(f\"train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"accuracy: {metrics['accuracy']}\")\n",
    "\n",
    "# model used: sentence-transformers/all-mpnet-base-v2\n",
    "# train dataset: 32 samples\n",
    "# accuracy: 0.873421052631579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Thats it, we have created a high-performing text-classification model with only `32` labeled samples or 8 samples per class using the SetFit approach. Our SetFit classifier achieved an accuracy of `0.873421052631579` on the test set. For comparison a regular model fine-tuned on the whole dataset (`12 000`) achieves a performance [~94%](https://huggingface.co/fabriceyhc/bert-base-uncased-ag_news) accuracy.\n",
    "\n",
    "This means you with 375x less data you lose only ~7% accuracy. ü§Ø\n",
    "\n",
    "This is huge! SetFit will help so many company to get started with text-classification and transformers, without the need to label a lot of data and compute power. Compared to LLM training s SetFit classifier takes less than 1 hour on a small GPU (NIVIDA T4) to train or less than $1 so to speak. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dev': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6dd96c16031089903d5a31ec148b80aeb0d39c32affb1a1080393235fbfa2fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
